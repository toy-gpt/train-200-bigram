{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Toy GPT Repository","text":"<p>This repo is an example of a next-token prediction model, illustrating how language models are trained and used.</p>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li><code>README.md</code> in the repository root for the home page</li> <li><code>SETUP.md</code> for setup and workflow</li> <li><code>SE_MANIFEST.toml</code> for intent, scope, and declared training corpus</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This page documents the public API of the package.</p> <p>Documented functions below are considered stable.</p>"},{"location":"api/#training-math","title":"Training Math","text":""},{"location":"api/#toy_gpt_train.math_training","title":"math_training","text":"<p>math_training.py - Mathematical utilities used during model training.</p> <p>This module contains reusable math functions used by the training and inference code in this repository.</p> <p>Scope: - Pure functions with no model, vocabulary, or artifact assumptions. - Reusable across unigram, bigram, and higher-context variants.</p> <p>These functions are intentionally simple and explicit to support inspection and debugging.</p>"},{"location":"api/#toy_gpt_train.math_training.argmax","title":"argmax","text":"<pre><code>argmax(values: list[float]) -&gt; int\n</code></pre> <p>Return the index of the maximum value in a list.</p> Concept <p>argmax is the argument (index) at which a function reaches its maximum.</p> Common uses <ul> <li>Measuring accuracy during training (pick the most likely token)</li> <li>Greedy decoding during inference (choose the top prediction)</li> </ul> In training and inference <ul> <li>A model outputs a probability distribution over possible next tokens.</li> <li>The token with the highest probability is the model's most confident prediction.</li> <li>argmax selects that token.</li> </ul> Example <p>values = [0.1, 0.7, 0.2] has index values of 0,1, 2 respectively. argmax(values) -&gt; 1 (since 0.7 is the largest value)</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[float]</code> <p>A non-empty list of numeric values (typically logits or probabilities).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the largest value in the list.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If values is empty.</p>"},{"location":"api/#toy_gpt_train.math_training.cross_entropy_loss","title":"cross_entropy_loss","text":"<pre><code>cross_entropy_loss(\n    probs: list[float], target_id: int\n) -&gt; float\n</code></pre> <p>Compute cross-entropy loss for a single training example.</p> Cross-Entropy Loss <p>Cross-entropy measures how well a predicted probability distribution matches the true outcome.</p> <p>In next-token prediction: - The true distribution is \"one-hot\" which means we encode it as either 1 or 0:     - Probability = 1.0 for the correct next token     - Probability = 0.0 for all others - The model predicts a probability distribution over all tokens.</p> <p>Cross-entropy answers the question:     \"How well does the predicted probability distribution align with the true outcome?\"</p> Formula <p>loss = -log(p_correct)</p> <ul> <li>If the model assigns high probability to the correct token,   the loss is small.</li> <li>If the probability is near zero, the loss is large.</li> </ul> Numerical safety <p>log(0) is undefined, so we clamp probabilities to a small minimum (1e-12). This does not change learning behavior in practice, but prevents runtime errors.</p> In training <ul> <li>This loss value drives gradient descent.</li> <li>Lower loss means better predictions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>list[float]</code> <p>A probability distribution over the vocabulary (sums to 1.0).</p> required <code>target_id</code> <code>int</code> <p>The integer ID of the correct next token.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A non-negative floating-point loss value.</p> <code>float</code> <ul> <li>0.0 means a perfect prediction</li> </ul> <code>float</code> <ul> <li>Larger values indicate worse predictions</li> </ul> <p>Raises:     ValueError: If target_id is out of range for probs.</p>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#toy_gpt_train.c_model","title":"c_model","text":"<p>c_model.py - Simple model module.</p> <p>Defines a minimal next-token prediction model using bigram context.   A bigram models P(next | current).</p> <p>Responsibilities: - Represent a simple parameterized model that maps a   token ID (current token) to a score for each token in the vocabulary. - Convert scores into probabilities using softmax. - Provide a forward pass (no training in this module).</p> <p>This model is intentionally simple: - one weight table (2D matrix: current x next) - one forward computation - no learning here</p> <p>Training is handled in a different module.</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel","title":"SimpleNextTokenModel","text":"<p>Bigram next-token prediction model.</p> <p>Maps (current_token_id) -&gt; distribution over next tokens.</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel.__init__","title":"__init__","text":"<pre><code>__init__(vocab_size: int) -&gt; None\n</code></pre> <p>Initialize the model with a given vocabulary size.</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel.forward","title":"forward","text":"<pre><code>forward(current_id: int) -&gt; list[float]\n</code></pre> <p>Perform a forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>current_id</code> <code>int</code> <p>Integer ID of the current token.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Probability distribution over next tokens.</p>"},{"location":"api/#toy_gpt_train.c_model.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Demonstrate a forward pass of the simple bigram model.</p>"},{"location":"api/#training-pipeline","title":"Training Pipeline","text":""},{"location":"api/#toy_gpt_train.d_train","title":"d_train","text":"<p>d_train.py - Training loop module.</p> <p>Trains the SimpleNextTokenModel on a small token corpus using a bigram context (current token \u2192 next token).</p> <p>Responsibilities: - Create (current_token -&gt; next_token) training pairs from the corpus - Run a basic gradient-descent training loop - Track loss and accuracy per epoch - Write a CSV log of training progress - Write inspectable training artifacts (vocabulary, weights, embeddings, meta)</p> <p>Concepts: - epoch: one complete pass through all training pairs - softmax: converts raw scores into probabilities (so predictions sum to 1) - cross-entropy loss: measures how well predicted probabilities match the correct token - gradient descent: iterative weight updates to minimize loss   - think descending to find the bottom of a valley in a landscape   - where the valley floor corresponds to lower prediction error</p> <p>Notes: - This is intentionally simple: no deep learning framework, no Transformer. - The model is a softmax regression classifier over bigram contexts. - Training updates the weight row corresponding to the observed current token. - token_embeddings.csv is a visualization-friendly projection for levels 100-400;   in later repos (500+), embeddings become a first-class learned table.</p>"},{"location":"api/#toy_gpt_train.d_train.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run a simple training demo end-to-end.</p>"},{"location":"api/#toy_gpt_train.d_train.make_training_pairs","title":"make_training_pairs","text":"<pre><code>make_training_pairs(\n    token_ids: list[int],\n) -&gt; list[BigramPair]\n</code></pre> <p>Convert token IDs into (current, next) pairs.</p>"},{"location":"api/#toy_gpt_train.d_train.row_labeler_bigram","title":"row_labeler_bigram","text":"<pre><code>row_labeler_bigram(\n    vocab: VocabularyLike, vocab_size: int\n) -&gt; RowLabeler\n</code></pre> <p>Map a bigram row index to a label (the token itself).</p>"},{"location":"api/#toy_gpt_train.d_train.token_row_index_bigram","title":"token_row_index_bigram","text":"<pre><code>token_row_index_bigram(\n    token_id: int, vocab_size: int\n) -&gt; int\n</code></pre> <p>Return the row index for a bigram context.</p> <p>For bigram, the row index is simply the token_id itself.</p>"},{"location":"api/#toy_gpt_train.d_train.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: SimpleNextTokenModel,\n    pairs: list[BigramPair],\n    learning_rate: float,\n    epochs: int,\n) -&gt; list[dict[str, float]]\n</code></pre> <p>Train the model using gradient descent on softmax cross-entropy.</p> <p>Training proceeds in epochs (full passes through all training pairs). For each pair, we: 1. Compute the model's predicted probabilities (forward pass). 2. Measure how wrong the prediction was (loss). 3. Adjust weights to reduce the loss (gradient descent).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SimpleNextTokenModel</code> <p>The model to train (weights will be modified in place).</p> required <code>pairs</code> <code>list[BigramPair]</code> <p>List of (current_id, target_id) training pairs.</p> required <code>learning_rate</code> <code>float</code> <p>Step size for gradient descent. Larger values learn faster but may overshoot; smaller values are more stable but slower.</p> required <code>epochs</code> <code>int</code> <p>Number of complete passes through the training data.</p> required <p>Returns:</p> Type Description <code>list[dict[str, float]]</code> <p>List of per-epoch metrics dictionaries containing epoch number,</p> <code>list[dict[str, float]]</code> <p>average loss, and accuracy.</p>"},{"location":"api/#inference","title":"Inference","text":""},{"location":"api/#toy_gpt_train.e_infer","title":"e_infer","text":"<p>e_infer.py - Inference module (artifact-driven).</p> <p>Runs inference using previously saved training artifacts.</p> <p>Responsibilities: - Load inspectable training artifacts from artifacts/   - 00_meta.json   - 01_vocabulary.csv   - 02_model_weights.csv - Reconstruct a vocabulary-like interface and model weights - Generate tokens using greedy decoding (argmax) - Print top-k next-token probabilities for inspection</p> <p>Notes: - This module does NOT retrain by default. - If artifacts are missing, run d_train.py first.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary","title":"ArtifactVocabulary  <code>dataclass</code>","text":"<p>Vocabulary reconstructed from artifacts/01_vocabulary.csv.</p> <p>Provides the same surface area used by inference: - vocab_size() - get_token_id() - get_id_token() - get_token_frequency()</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_id_token","title":"get_id_token","text":"<pre><code>get_id_token(idx: int) -&gt; str | None\n</code></pre> <p>Return the token for a given token ID, or None if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_token_frequency","title":"get_token_frequency","text":"<pre><code>get_token_frequency(token: str) -&gt; int\n</code></pre> <p>Return the frequency count for a given token, or 0 if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_token_id","title":"get_token_id","text":"<pre><code>get_token_id(token: str) -&gt; int | None\n</code></pre> <p>Return the token ID for a given token, or None if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.vocab_size","title":"vocab_size","text":"<pre><code>vocab_size() -&gt; int\n</code></pre> <p>Return the total number of tokens in the vocabulary.</p>"},{"location":"api/#toy_gpt_train.e_infer.generate_tokens_bigram","title":"generate_tokens_bigram","text":"<pre><code>generate_tokens_bigram(\n    model: SimpleNextTokenModel,\n    vocab: ArtifactVocabulary,\n    start_token: str,\n    num_tokens: int,\n) -&gt; list[str]\n</code></pre> <p>Generate tokens using a bigram context (current token \u2192 next token).</p>"},{"location":"api/#toy_gpt_train.e_infer.load_meta","title":"load_meta","text":"<pre><code>load_meta(path: Path) -&gt; JsonObject\n</code></pre> <p>Load 00_meta.json.</p>"},{"location":"api/#toy_gpt_train.e_infer.load_model_weights_csv","title":"load_model_weights_csv","text":"<pre><code>load_model_weights_csv(\n    path: Path, vocab_size: int, *, expected_rows: int\n) -&gt; list[list[float]]\n</code></pre> <p>Load 02_model_weights.csv -&gt; weights matrix.</p> <p>Expected shape: - one row per input token (current token for bigram model) - one column per output token (after the first 'input_token' column)</p>"},{"location":"api/#toy_gpt_train.e_infer.load_vocabulary_csv","title":"load_vocabulary_csv","text":"<pre><code>load_vocabulary_csv(path: Path) -&gt; ArtifactVocabulary\n</code></pre> <p>Load 01_vocabulary.csv -&gt; ArtifactVocabulary.</p>"},{"location":"api/#toy_gpt_train.e_infer.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run inference using saved training artifacts.</p>"},{"location":"api/#toy_gpt_train.e_infer.require_artifacts","title":"require_artifacts","text":"<pre><code>require_artifacts(\n    *,\n    meta_path: Path,\n    vocab_path: Path,\n    weights_path: Path,\n    train_hint: str,\n) -&gt; None\n</code></pre> <p>Fail fast with a helpful message if artifacts are missing.</p>"},{"location":"api/#toy_gpt_train.e_infer.top_k","title":"top_k","text":"<pre><code>top_k(\n    probs: list[float], k: int\n) -&gt; list[tuple[int, float]]\n</code></pre> <p>Return top-k (token_id, probability) pairs sorted by probability.</p>"},{"location":"api/#artifacts-and-io","title":"Artifacts and I/O","text":""},{"location":"api/#toy_gpt_train.io_artifacts","title":"io_artifacts","text":"<p>io_artifacts.py - Input/output and training-artifact utilities used by the models.</p> <p>This module is responsible for persisting and describing the results of model training in a consistent, inspectable format.</p> <p>It does not perform training. It: - Writes artifacts produced by training (weights, vocabulary, logs, metadata) - Assumes a conventional repository layout for reproducibility - Provides small helper utilities shared across training and inference</p> <p>The expected directory structure is: - artifacts/ contains all inspectable model outputs - corpus/ contains training text files (often exactly one) - outputs/ contains training logs and diagnostics</p> <p>External callers should treat paths as implementation details and interact through the functions provided here.</p>"},{"location":"api/#toy_gpt_train.io_artifacts--concepts","title":"Concepts","text":"<p>Artifact     A concrete file written to disk that captures some aspect of training.     In this project, artifacts are designed to be:     - Human-readable (CSV / JSON)     - Stable across model variants (unigram, bigram, context-3, etc.)     - Reusable by inference without retraining</p> <p>Epoch     One epoch is one complete pass through all training examples.     Training typically consists of multiple epochs so the model can     gradually improve its predictions by repeatedly adjusting weights.</p> <p>Training Log     A CSV file recording per-epoch metrics such as:     - average loss     - accuracy     This allows learning behavior to be inspected after training.</p> <p>Vocabulary     A mapping between token strings and integer token IDs.     The vocabulary defines:     - the size of the model output space     - the meaning of each row and column in the weight tables</p> <p>Row Labeler     A small function that maps a numeric row index in the model's weight table     to a human-readable label.     For example, as the number of context tokens increases, the row labeler     produces context strings such as:     - unigram:        \"cat\"     - bigram:         \"the|cat\"     - context-3:      \"the|black|cat\"</p> <pre><code>Row labels are written into CSV artifacts to make model structure visible.\n</code></pre> <p>Model Weights     Numeric parameters learned during training.     Conceptually:     - each row corresponds to an input context     - each column corresponds to a possible next token     Weights are written verbatim so learning can be inspected or reused.</p> <p>Token Embeddings (Derived)     A simple 2D projection derived from model weights for visualization.     These are not learned embeddings yet.     In later stages (500+), embeddings become first-class learned parameters.</p> <p>Reproducibility Metadata     The 00_meta.json file records:     - which corpus was used     - how it was hashed     - which model variant was trained     - what training settings were applied     This allows results to be traced and compared across runs and repositories.</p>"},{"location":"api/#toy_gpt_train.io_artifacts--design-notes","title":"Design Notes","text":"<ul> <li>This module is shared unchanged across model levels (100-400).</li> <li>More advanced pipelines (embeddings, attention, batching) build on   the same artifact-writing concepts.</li> <li>Centralizing I/O logic prevents drift across repositories   and keeps training code focused on learning.</li> </ul>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike","title":"VocabularyLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for vocabulary-like objects used in training and artifacts.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_id_token","title":"get_id_token","text":"<pre><code>get_id_token(idx: int) -&gt; str | None\n</code></pre> <p>Return the token string for a given integer ID, or None if not found.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_token_frequency","title":"get_token_frequency","text":"<pre><code>get_token_frequency(token: str) -&gt; int\n</code></pre> <p>Return the frequency count for a given token.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_token_id","title":"get_token_id","text":"<pre><code>get_token_id(token: str) -&gt; int | None\n</code></pre> <p>Return the integer ID for a given token, or None if not found.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.vocab_size","title":"vocab_size","text":"<pre><code>vocab_size() -&gt; int\n</code></pre> <p>Return the total number of unique tokens in the vocabulary.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.artifact_paths_from_base_dir","title":"artifact_paths_from_base_dir","text":"<pre><code>artifact_paths_from_base_dir(\n    base_dir: Path,\n) -&gt; dict[str, Path]\n</code></pre> <p>Return standard artifact paths under base_dir/artifacts/.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.artifacts_dir_from_base_dir","title":"artifacts_dir_from_base_dir","text":"<pre><code>artifacts_dir_from_base_dir(base_dir: Path) -&gt; Path\n</code></pre> <p>Return artifacts/ directory under a repository base directory.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.find_single_corpus_file","title":"find_single_corpus_file","text":"<pre><code>find_single_corpus_file(corpus_dir: Path) -&gt; Path\n</code></pre> <p>Find the single corpus file in corpus/ (same rule as SimpleTokenizer).</p>"},{"location":"api/#toy_gpt_train.io_artifacts.outputs_dir_from_base_dir","title":"outputs_dir_from_base_dir","text":"<pre><code>outputs_dir_from_base_dir(base_dir: Path) -&gt; Path\n</code></pre> <p>Return outputs/ directory under a repository base directory.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.repo_name_from_base_dir","title":"repo_name_from_base_dir","text":"<pre><code>repo_name_from_base_dir(base_dir: Path) -&gt; str\n</code></pre> <p>Infer repository name from base directory.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.sha256_of_bytes","title":"sha256_of_bytes","text":"<pre><code>sha256_of_bytes(data: bytes) -&gt; str\n</code></pre> <p>Return hex SHA-256 digest for given bytes.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.sha256_of_file","title":"sha256_of_file","text":"<pre><code>sha256_of_file(path: Path) -&gt; str\n</code></pre> <p>Return hex SHA-256 digest for a file.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.write_artifacts","title":"write_artifacts","text":"<pre><code>write_artifacts(\n    *,\n    base_dir: Path,\n    corpus_path: Path,\n    vocab: VocabularyLike,\n    model: SimpleNextTokenModel,\n    model_kind: str,\n    learning_rate: float,\n    epochs: int,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write all training artifacts to artifacts/.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>Path</code> <p>Repository base directory.</p> required <code>corpus_path</code> <code>Path</code> <p>Corpus file used for training.</p> required <code>vocab</code> <code>VocabularyLike</code> <p>VocabularyLike instance.</p> required <code>model</code> <code>SimpleNextTokenModel</code> <p>Trained model (weights already updated).</p> required <code>model_kind</code> <code>str</code> <p>Human-readable model kind (e.g., \"unigram\", \"bigram\").</p> required <code>learning_rate</code> <code>float</code> <p>Training learning rate.</p> required <code>epochs</code> <code>int</code> <p>Number of training passes.</p> required <code>row_labeler</code> <code>RowLabeler</code> <p>Function that maps a model weight-row index to a label written in the first column of 02_model_weights.csv.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_meta_json","title":"write_meta_json","text":"<pre><code>write_meta_json(\n    path: Path,\n    *,\n    base_dir: Path,\n    corpus_path: Path,\n    vocab_size: int,\n    model_kind: str,\n    learning_rate: float,\n    epochs: int,\n) -&gt; None\n</code></pre> <p>Write 00_meta.json describing corpus, model, and training settings.</p> <p>This file is the authoritative, human-readable summary of a training run. It records: - what corpus was used - what model architecture was trained - how training was configured - which artifacts were produced</p> <p>The intent is transparency and reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output JSON path (artifacts/00_meta.json).</p> required <code>base_dir</code> <code>Path</code> <p>Repository base directory.</p> required <code>corpus_path</code> <code>Path</code> <p>Corpus file used for training.</p> required <code>vocab_size</code> <code>int</code> <p>Number of unique tokens.</p> required <code>model_kind</code> <code>str</code> <p>Human-readable model kind (e.g., \"unigram\", \"bigram\", \"context2\", \"context3\").</p> required <code>learning_rate</code> <code>float</code> <p>Training learning rate.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs (full passes over the training pairs).</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_model_weights_csv","title":"write_model_weights_csv","text":"<pre><code>write_model_weights_csv(\n    path: Path,\n    vocab: VocabularyLike,\n    model: SimpleNextTokenModel,\n    *,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write 02_model_weights.csv with token-labeled columns.</p> Shape <ul> <li>first column: input_token (serialized context label)</li> <li>remaining columns: one per output token (weights)</li> </ul> Notes <ul> <li>Tokens may be typed internally.</li> <li>This function serializes all token labels explicitly at the I/O boundary.</li> </ul>"},{"location":"api/#toy_gpt_train.io_artifacts.write_token_embeddings_csv","title":"write_token_embeddings_csv","text":"<pre><code>write_token_embeddings_csv(\n    path: Path,\n    model: SimpleNextTokenModel,\n    *,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write 03_token_embeddings.csv as a simple 2D projection.</p> <p>This file is a derived visualization artifact, not a learned embedding table.</p> For each model weight row <ul> <li>x coordinate = first weight (if present)</li> <li>y coordinate = second weight (if present)</li> </ul> <p>If a row has fewer than two weights, missing values default to 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output CSV path.</p> required <code>model</code> <code>SimpleNextTokenModel</code> <p>Trained model providing a weight matrix.</p> required <code>row_labeler</code> <code>RowLabeler</code> <p>Function mapping row index to a human-readable label.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_training_log","title":"write_training_log","text":"<pre><code>write_training_log(\n    path: Path, history: list[dict[str, float]]\n) -&gt; None\n</code></pre> <p>Write per-epoch training metrics to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path.</p> required <code>history</code> <code>list[dict[str, float]]</code> <p>List of per-epoch metrics dictionaries.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_vocabulary_csv","title":"write_vocabulary_csv","text":"<pre><code>write_vocabulary_csv(\n    path: Path, vocab: VocabularyLike\n) -&gt; None\n</code></pre> <p>Write 01_vocabulary.csv: token_id, token, frequency.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output CSV path.</p> required <code>vocab</code> <code>VocabularyLike</code> <p>Vocabulary instance (must provide vocab_size(), get_id_token(), get_token_frequency()).</p> required"},{"location":"api/#marker-file-pytyped","title":"Marker File: py.typed","text":"<p>This package includes a <code>py.typed</code> marker file as defined by PEP 561.</p> <ul> <li>Type checkers (Pyright, Mypy) trust inline type hints in installed packages only when this marker is present.</li> <li>The file may be empty; comments are allowed.</li> </ul>"}]}