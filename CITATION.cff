# ============================================================
# CITATION.cff (How to cite this project)
# ============================================================

cff-version: "1.2.0"
type: software

title: "Toy-GPT: Bigram Next-Token Language Model"
version: "0.9.0"
date-released: "2026-01-16"

authors:
  - family-names: Case
    given-names: Denise M.
    orcid: "https://orcid.org/0000-0001-6165-7389"
    affiliation: "Northwest Missouri State University, School of Computer Science and Information Systems, Maryville, MO, USA"

repository-code: "https://github.com/toy-gpt/train-200-bigram"
url: "https://github.com/toy-gpt/train-200-bigram"
license: MIT

abstract: >
  Toy-GPT is an educational sequence of minimal language-model training repositories.
  This repository implements a bigram next-token prediction model trained with
  softmax and cross-entropy loss. It serves as a baseline for understanding
  probabilistic language modeling, training loops, and inspectable model artifacts
  prior to introducing higher-order context, embeddings, or attention mechanisms.

keywords:
  # domain
  - language-models
  - natural-language-processing
  - probabilistic-models
  - next-token-prediction

  # pedagogy
  - education
  - computer-science-education
  - machine-learning-education
  - teaching
  - learning

  # methodology
  - bigram-model
  - softmax
  - cross-entropy-loss
  - gradient-descent

  # software practice
  - python
  - reproducible-research
  - inspectable-artifacts
  - open-source
  - github

message: "If you use this software in academic or educational work, please cite it using this file."
